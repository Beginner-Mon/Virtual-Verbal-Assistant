# Virtual Verbal Assistant (SpeechSLM)

This project implements a multimodal virtual assistant that accepts
either **speech or text input**, detects the user's **emotion**, formats
the interaction into a structured JSON representation, and generates
responses using a **Small Language Model (SLM)** running locally.

The assistant then outputs both **text and speech**, and can optionally
control a virtual avatar.

---

## ğŸ¯ Features

- Speech or text input
- Speech-to-Text (Whisper / wav2vec2)
- Emotion recognition from text/audio
- Context formatting using JSON
- Local SLM inference (Phi / Mistral / Gemma via Ollama)
- Text-to-Speech output
- Modular system architecture

---

## ğŸ§  System Pipeline

Voice / Text Input
â†“
Speech-to-Text
â†“
Emotion Detection
â†“
Context Formatter (JSON)
â†“
SLM Reasoning Engine
â†“
Text-to-Speech + Avatar

---

## ğŸ“ Project Structure

SpeechLLM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                   # The "Brain" and State Logic
â”‚   â”‚   â”œâ”€â”€ orchestrator.py      # Main loop; moves data between STT -> LLM -> TTS
â”‚   â”‚   â”œâ”€â”€ state_machine.py     # Tracks IDLE, LISTENING, THINKING, SPEAKING
â”‚   â”‚   â””â”€â”€ events.py            # Signals for "Interrupt" or "Speech Detected"
â”‚   â”‚
â”‚   â”œâ”€â”€ stages/                  # Pipeline Steps (The "What")
â”‚   â”‚   â”œâ”€â”€ stt_stage.py         # Takes audio buffer -> returns text
â”‚   â”‚   â”œâ”€â”€ llm_stage.py         # Takes text + emotion -> returns Phi-3 response
â”‚   â”‚   â”œâ”€â”€ tts_stage.py         # Takes text -> returns audio stream
â”‚   â”‚   â””â”€â”€ emotion_stage.py     # Analyzes text/audio for emotional metadata
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                # Model Wrappers (The "How")
â”‚   â”‚   â”œâ”€â”€ phi3_client.py       # Specific implementation for Phi-3 inference
â”‚   â”‚   â”œâ”€â”€ whisper_client.py    # Faster-Whisper or OpenAI Whisper setup
â”‚   â”‚   â””â”€â”€ voice_driver.py      # Low-level audio device management (PyAudio)
â”‚   â”‚
â”‚   â”œâ”€â”€ context/                 # Memory & Formatting
â”‚   â”‚   â”œâ”€â”€ memory_manager.py    # Handles conversation history (Short-term/Long-term)
â”‚   â”‚   â””â”€â”€ prompt_templates.py  # Stores system prompts and JSON schemas
â”‚   â”‚
â”‚   â”œâ”€â”€ ui/                      # Visuals
â”‚   â”‚   â”œâ”€â”€ avatar_controller.py # Sends blendshapes/visemes to the 3D model
â”‚   â”‚   â””â”€â”€ web_server.py        # FastAPI/WebSocket for the frontend dashboard
â”‚   â”‚
â”‚   â””â”€â”€ utils/                   # Helpers
â”‚       â”œâ”€â”€ logger.py            # Custom logging to console and file
â”‚       â””â”€â”€ audio_tools.py       # VAD (Voice Activity Detection) and Chunking
â”‚
â”œâ”€â”€ configs/                     # System Settings
â”‚   â”œâ”€â”€ base.yaml                # Default parameters (Sample rates, etc.)
â”‚   â””â”€â”€ models.yaml              # Paths to local .bin or .onnx files
â”‚
â”œâ”€â”€ models/                      # Weights (Git-ignored)
â”‚   â”œâ”€â”€ phi3/                    # Phi-3-3.8B files
â”‚   â””â”€â”€ whisper/                 # Whisper weights
â”‚
â”œâ”€â”€ data/                        # Persistent Data
â”‚   â”œâ”€â”€ logs/                    # Runtime logs for debugging
â”‚   â””â”€â”€ temp_audio/              # Cache for temporary .wav processing
â”‚
â”œâ”€â”€ .env                         # Environment variables (API keys)
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ main.py                      # Application Entry Point
â””â”€â”€ README.md                    # Setup and usage guide


---

## âš™ï¸ Setup

1. Create Conda Environment

```bash
conda env create -f environment.yml
conda activate speechslm

2. Install Ollama (for SLM)
Download from:

https://ollama.com

Pull a model:

ollama pull phi3
or:

ollama pull mistral
3. Run
python main.py
ğŸ§ª Example Output
User (speech): I feel stressed today.
Emotion: anxious (0.81)

Model: I'm sorry you're feeling stressed. Want to talk about what's causing it?

---

## ğŸ“Š Project Goals

Demonstrate multimodal interaction

Explore emotion-aware dialogue systems

Compare SLM performance vs large LLMs

Evaluate latency and accuracy

Control avatar expressions

--- 

ğŸ“„ License

Educational use only.


---
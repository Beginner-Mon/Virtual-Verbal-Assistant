# AgenticRAG â€” Developer Documentation

> **Last updated**: 2026-02-28  
> **Python**: 3.13+ &nbsp;|&nbsp; **LLM**: Google Gemini 2.5 Flash &nbsp;|&nbsp; **Vector DB**: ChromaDB 1.5  
> **Status**: Active development

---

## Table of Contents

1. [System Overview](#system-overview)
2. [High-Level Architecture](#high-level-architecture)
3. [Project Structure](#project-structure)
4. [Module Reference](#module-reference)
5. [Agentic Pipeline â€” Detailed Flow](#agentic-pipeline--detailed-flow)
6. [Vector Database & Chunking Architecture](#vector-database--chunking-architecture)
7. [Session & Memory Management](#session--memory-management)
8. [Web Search Fallback](#web-search-fallback)
9. [API Key Rotation System](#api-key-rotation-system)
10. [Prompt Engineering](#prompt-engineering)
11. [Configuration Reference](#configuration-reference)
12. [Environment Variables](#environment-variables)
13. [Setup & Installation](#setup--installation)
14. [Running the System](#running-the-system)
15. [Testing](#testing)
16. [How to Extend the System](#how-to-extend-the-system)
17. [Dependency Matrix](#dependency-matrix)
18. [Resolved Issues & Fixes](#resolved-issues--fixes)
19. [Known Limitations & Open Issues](#known-limitations--open-issues)
20. [Production Deployment Checklist](#production-deployment-checklist)

---

## System Overview

**AgenticRAG** (internally branded *KineticChat*) is an intelligent conversational AI system built on a **Retrieval-Augmented Generation** (RAG) architecture with **agentic decision-making**. The system:

- ğŸ¤– **Routes queries intelligently** â€” An Orchestrator Agent (powered by Gemini) analyzes intent and decides the action plan before any retrieval or generation happens.
- ğŸ“š **Multi-source knowledge** â€” Retrieves context from uploaded documents (PDF, Word, Images w/ OCR), conversation memory, chat session summaries, and live web search.
- ğŸ” **Self-correcting pipeline** â€” Implements query reformulation (rewrites poor queries), iterative reflection (verifies answers against sources), and web search fallback.
- ğŸ’¾ **Persistent memory** â€” Stores conversations, documents, and session summaries in ChromaDB with semantic vector search.
- ğŸ”‘ **Multi-key API management** â€” Rotates across multiple Gemini API keys automatically on quota errors.
- ğŸŒ **Streamlit web UI** â€” Full-featured chat interface with document upload, session management, and sidebar controls.

### Example Use Case

Upload course documents (PDFs, Word files) and ask questions about their content. The system remembers past conversations, searches across documents semantically, and falls back to web search when local knowledge is insufficient.

---

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER (Streamlit UI / CLI)                       â”‚
â”‚                    ui.py  Â·  run_ui.py  Â·  main.py                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ query + uploaded files             â”‚ response
                      â–¼                                    â–²
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ORCHESTRATOR AGENT               agents/orchestrator.py            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚ Gemini 2.5 Flash (temp=0.1)                        â”‚              â”‚
â”‚     â”‚ Analyzes query â†’ decides action plan (JSON)        â”‚              â”‚
â”‚     â”‚ Actions: RETRIEVE_DOCUMENT | RETRIEVE_MEMORY |     â”‚              â”‚
â”‚     â”‚          CALL_LLM | HYBRID | CLARIFY               â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. RAG PIPELINE                     retrieval/rag_pipeline.py         â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Query        â”‚â†’ â”‚ Retrieve     â”‚â†’ â”‚ Quality      â”‚â†’ â”‚ Build      â”‚  â”‚
â”‚  â”‚ Expansion    â”‚  â”‚ Context      â”‚  â”‚ Assessment   â”‚  â”‚ Prompt     â”‚  â”‚
â”‚  â”‚ (LLM)       â”‚  â”‚ (hybrid)     â”‚  â”‚ + Reformulateâ”‚  â”‚ + Generate â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                 â”‚                â”‚          â”‚
â”‚                           â–¼                 â–¼                â–¼          â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                    â”‚ Web Search  â”‚   â”‚ Iterative  â”‚   â”‚ Response   â”‚   â”‚
â”‚                    â”‚ Fallback    â”‚   â”‚ Reflection â”‚   â”‚ Validation â”‚   â”‚
â”‚                    â”‚ (DuckDuckGo)â”‚   â”‚ (grounding)â”‚   â”‚ (safety)   â”‚   â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. MEMORY LAYER                                                        â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Memory Manager  â”‚  â”‚ Document Store  â”‚  â”‚ Session Store           â”‚ â”‚
â”‚  â”‚ (conversations) â”‚  â”‚ (doc chunks)    â”‚  â”‚ (JSON-on-disk sessions) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                    â”‚                         â”‚              â”‚
â”‚           â–¼                    â–¼                         â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    Vector Store (ChromaDB)                      â”‚    â”‚
â”‚  â”‚  Collections:                                                   â”‚    â”‚
â”‚  â”‚  â€¢ kinetichat_memory           (conversations)                  â”‚    â”‚
â”‚  â”‚  â€¢ kinetichat_memory_documents (document chunks)                â”‚    â”‚
â”‚  â”‚  â€¢ kinetichat_chat_summaries   (session summaries)              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Embedding       â”‚  â”‚ Summarize Agent                          â”‚     â”‚
â”‚  â”‚ Service         â”‚  â”‚ (condenses sessions â†’ vector summaries)  â”‚     â”‚
â”‚  â”‚ (MiniLM-L6-v2)  â”‚  â”‚ agents/summarize_agent.py                â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. UTILITIES                                                           â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Gemini Client â”‚ â”‚ API Key Mgr   â”‚ â”‚ Document     â”‚ â”‚ Logger      â”‚  â”‚
â”‚  â”‚ (OpenAI-compatâ”‚ â”‚ (multi-key    â”‚ â”‚ Loader       â”‚ â”‚ (loguru)    â”‚  â”‚
â”‚  â”‚  wrapper)     â”‚ â”‚  rotation)    â”‚ â”‚ (PDF/OCR)    â”‚ â”‚             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Web Search    â”‚ â”‚ Validators    â”‚ â”‚ Prompt       â”‚                  â”‚
â”‚  â”‚ (DuckDuckGo)  â”‚ â”‚ (safety)      â”‚ â”‚ Templates    â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
agentic_rag_gemini/
â”‚
â”œâ”€â”€ main.py                          # CLI entry point (interactive / single-query mode)
â”œâ”€â”€ ui.py                            # Streamlit web interface (951 lines)
â”œâ”€â”€ run_ui.py                        # Streamlit launcher helper
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ setup.ps1                        # Windows PowerShell setup script
â”œâ”€â”€ setup.sh                         # Linux/macOS setup script
â”œâ”€â”€ .env.example                     # Environment variable template
â”œâ”€â”€ .env                             # API keys (not committed)
â”œâ”€â”€ .gitignore                       # Git ignore rules
â”œâ”€â”€ QUICKSTART.md                    # 5-minute user setup guide
â”œâ”€â”€ README_DEVELOPERS.md             # This file
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py                  # Pydantic config models + YAML loader
â”‚   â””â”€â”€ config.yaml                  # Main configuration (226 lines)
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ orchestrator.py              # Query routing agent (340 lines)
â”‚   â””â”€â”€ summarize_agent.py           # Chat session summarizer (144 lines)
â”‚
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ rag_pipeline.py              # RAG pipeline with agentic loops (759 lines)
â”‚
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ memory_manager.py            # Conversation memory CRUD (555 lines)
â”‚   â”œâ”€â”€ document_store.py            # Document chunking & search (399 lines)
â”‚   â”œâ”€â”€ vector_store.py              # ChromaDB/Qdrant wrapper (765 lines)
â”‚   â”œâ”€â”€ embedding_service.py         # Sentence-Transformers + Gemini embeddings (261 lines)
â”‚   â””â”€â”€ session_store.py             # JSON-on-disk chat session persistence (252 lines)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gemini_client.py             # OpenAI-compatible Gemini wrapper (517 lines)
â”‚   â”œâ”€â”€ api_key_manager.py           # Multi-key rotation with round-robin (267 lines)
â”‚   â”œâ”€â”€ document_loader.py           # PDF/Word/Image/OCR loader (409 lines)
â”‚   â”œâ”€â”€ web_search.py                # DuckDuckGo search service (232 lines)
â”‚   â”œâ”€â”€ prompt_templates.py          # All prompt templates (430 lines)
â”‚   â”œâ”€â”€ validators.py                # Response safety validation (283 lines)
â”‚   â””â”€â”€ logger.py                    # Loguru-based logging (140 lines)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_orchestrator.py         # Orchestrator unit tests
â”‚   â””â”€â”€ test_session_store.py        # Session store unit tests
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ vector_store/                # ChromaDB persistent storage
â”‚   â””â”€â”€ sessions/                    # Chat session JSON files
â”‚
â””â”€â”€ logs/
    â””â”€â”€ agentic_rag.log              # Application logs (rotated at 10MB)
```

---

## Module Reference

### 1. Orchestrator Agent â€” `agents/orchestrator.py`

The "brain" of the system. Uses Gemini (temperature=0.1) to analyze user queries and produce structured JSON routing decisions.

| Class / Function | Purpose |
|---|---|
| `ActionType` (enum) | `RETRIEVE_MEMORY`, `CALL_LLM`, `GENERATE_MOTION`, `HYBRID`, `CLARIFY` |
| `OrchestratorDecision` | Data class holding action, confidence, reasoning, parameters |
| `OrchestratorAgent.analyze_query()` | Calls Gemini to classify intent â†’ returns `OrchestratorDecision` |
| `OrchestratorAgent.process_query()` | High-level entry: analyze â†’ return action plan dict |
| `OrchestratorAgent._build_analysis_prompt()` | Constructs the system+user prompt for the routing LLM |
| `OrchestratorAgent.should_retrieve_memory()` | Decision helper: should memory be fetched? |
| `OrchestratorAgent.should_call_llm()` | Decision helper: should LLM generate a response? |
| `clean_json_response()` | Strips markdown fences / fixes malformed JSON from LLM |

**Key design choice**: The orchestrator is a *pure router* â€” it never generates user-facing text. Its output is a JSON routing decision consumed by `main.py` or `ui.py`.

---

### 2. Summarize Agent â€” `agents/summarize_agent.py`

Condenses chat session transcripts into concise summaries that are embedded into ChromaDB for cross-session recall.

| Method | Purpose |
|---|---|
| `summarize_session()` | Calls Gemini to generate a 3â€“5 sentence summary of a message list |
| `store_summary()` | Embeds the summary vector into the `chat_summaries` collection |
| `summarize_and_store()` | Convenience: summarize + embed in one call |

**Trigger**: Called from `ui.py._summarize_current_session()` when the user switches to a new chat session.

---

### 3. RAG Pipeline â€” `retrieval/rag_pipeline.py`

The core response-generation engine. Implements a multi-step agentic pipeline:

| Method | Pipeline Step |
|---|---|
| `generate_response()` | **Orchestrates the full pipeline** (see Agentic Pipeline below) |
| `_process_query()` | Step 1: Query expansion via LLM |
| `_retrieve_context()` | Step 2: Hybrid retrieval (memory + documents + session summaries) |
| `_assess_context_quality()` | Step 3: Compute average similarity score |
| `_reformulate_query()` | Step 3b: LLM-based query rewrite if quality is low |
| `_build_prompt()` | Step 4: Construct full prompt with context, history, and instructions |
| `_generate_llm_response()` | Step 5: Call Gemini to generate answer |
| `_reflect_on_response()` | Step 6: LLM self-check for grounding/hallucination |
| `_retry_generation()` | Step 7: Retry with validation feedback if needed |

---

### 4. Memory Manager â€” `memory/memory_manager.py`

CRUD layer for conversation-level memory.

| Method | Purpose |
|---|---|
| `store_interaction()` | Saves a userâ†”assistant turn as an embedded vector |
| `retrieve_relevant_memory()` | Semantic search over past interactions |
| `get_recent_interactions()` | Chronological fetch of recent turns |
| `store_user_context()` | Stores user preferences, physical state, constraints |
| `get_user_profile()` | Aggregates user info from memory |
| `load_documents_from_file()` | âš ï¸ **Deprecated** â€” forwards to DocumentStore |
| `load_documents_from_directory()` | âš ï¸ **Deprecated** â€” forwards to DocumentStore |
| `clear_memory()` | Resets in-memory counters and state |
| `_create_summary()` | Periodic conversation summarization |

---

### 5. Document Store â€” `memory/document_store.py`

Manages uploaded documents with intelligent chunking and deduplication.

| Method | Purpose |
|---|---|
| `store_document()` | Routes to single-chunk or chunked storage based on doc size |
| `_store_single_document()` | Stores small docs (< `min_chunk_size`) as one vector |
| `_store_chunked_document()` | Splits large docs into overlapping chunks with rich metadata |
| `search_documents()` | Semantic search with chunk deduplication |
| `_deduplicate_chunks()` | Keeps top-N chunks per document to prevent result domination |
| `get_user_documents()` | Lists all documents for a user |
| `delete_document()` / `delete_user_documents()` | Cleanup operations |
| `count_documents()` | Document count (optionally per user) |

---

### 6. Vector Store â€” `memory/vector_store.py`

Low-level ChromaDB (and Qdrant stub) wrapper managing three collections.

| Collection | Contents | Collection Name |
|---|---|---|
| Conversations | Userâ†”assistant interaction embeddings | `kinetichat_memory` |
| Documents | Uploaded document chunk embeddings | `kinetichat_memory_documents` |
| Chat Summaries | Session summary embeddings | `kinetichat_chat_summaries` |

| Key Method | Purpose |
|---|---|
| `_init_chromadb()` | Initializes `PersistentClient` + creates/gets all 3 collections |
| `add_documents()` | Batch add to conversations or documents collection |
| `search()` | Similarity search on conversations with proper filter handling |
| `search_documents()` | Similarity search on document chunks |
| `add_chat_summary()` / `search_chat_summaries()` | CRUD for session summaries |
| `clear_all_data()` | Wipes all 3 collections with verification |
| `reset_collections()` | Delete + recreate collections (for schema fixes) |

**Similarity Calculation** (critical fix):
```python
# ChromaDB returns squared L2 distance. Conversion to cosine similarity:
similarity = max(0.0, 1.0 - (distance / 2.0))
```

---

### 7. Embedding Service â€” `memory/embedding_service.py`

Text â†’ vector conversion using Sentence Transformers (local) or Gemini (API).

| Property | Value |
|---|---|
| Default model | `sentence-transformers/all-MiniLM-L6-v2` |
| Embedding dimension | 384 |
| Batch size | 32 |
| Normalization | Disabled (to work with ChromaDB's L2 distance) |

| Method | Purpose |
|---|---|
| `embed_texts()` | Main entry â€” single text or batch |
| `compute_similarity()` | Cosine similarity between two vectors |
| `count_tokens()` / `truncate_text()` | Token management via tiktoken |

---

### 8. Session Store â€” `memory/session_store.py`

JSON-on-disk persistence for chat sessions. Each session is a JSON file under `data/sessions/{user_id}/`.

| Method | Purpose |
|---|---|
| `create_session()` | Creates a new empty session file |
| `save_turn()` | Appends a message to a session |
| `load_session()` | Reads full session data |
| `list_sessions()` | Returns sessions sorted by most-recently-updated |
| `delete_oldest_sessions()` | Prunes sessions beyond a keep limit |
| `mark_summarized()` | Stores summary text and flags session as summarized |

**Session file format**:
```json
{
  "session_id": "uuid",
  "title": "Auto-generated from first message",
  "created_at": "ISO-8601",
  "updated_at": "ISO-8601",
  "is_summarized": false,
  "summary": null,
  "messages": [
    {"role": "user", "content": "...", "timestamp": "..."},
    {"role": "assistant", "content": "...", "timestamp": "...", "metadata": {}}
  ]
}
```

---

### 9. Gemini Client â€” `utils/gemini_client.py`

OpenAI-compatible wrapper around Google's Gemini API.

| Class | Purpose |
|---|---|
| `GeminiClient` | Core API wrapper with auto-retry and key rotation |
| `GeminiClientWrapper` | Top-level interface exposing `GeminiClientWrapper.generate()` and `GeminiClientWrapper.chat.completions.create()` |
| `GeminiChatCompletion` | Mimics `client.chat.completions.create()` (OpenAI interface) |
| `GeminiResponse` / `GeminiChoice` / `GeminiMessage` | Response objects matching OpenAI's structure |

**Key features**:
- Automatic API key rotation on 429 (quota exceeded) errors
- Converts OpenAI-style `messages` list to Gemini's `contents` format
- System prompt extracted from messages and passed as `system_instruction`
- Supports `response_format` parameter for JSON mode

---

### 10. API Key Manager â€” `utils/api_key_manager.py`

Thread-safe singleton that manages multiple Gemini API keys with round-robin rotation.

```
GEMINI_API_KEYS=key1,key2,key3  # Comma-separated in .env
```

| Method | Purpose |
|---|---|
| `get_current_key()` | Returns the active API key |
| `rotate_to_next_key()` | Moves to next key (circular). Returns `False` after 2 full cycles |
| `mark_key_failed()` | Increments failure counter |
| `reset_success()` | Resets failure counter after successful call |
| `has_available_keys()` | Checks if more retries are allowed |

**Rotation logic**: Allows `2 Ã— total_keys` consecutive failures before declaring all keys exhausted.

---

### 11. Document Loader â€” `utils/document_loader.py`

Multi-format document text extraction.

| Format | Method | Features |
|---|---|---|
| PDF | `_load_pdf()` | Text extraction via pypdf + OCR via pdf2image + pytesseract |
| PDF (scanned) | `_ocr_pdf()` / `_ocr_pdf_images()` | Full-page OCR for scanned documents |
| Word (.docx) | `_load_docx()` | Extracts paragraphs + tables |
| Images | `_load_image()` | OCR via pytesseract |
| Text (.txt) | `_load_text()` | Plain text reading |

**PDF Strategy**: Always extracts regular text first, then OCR-renders each page as an image to catch diagrams, screenshots, and figures. Both sources are merged.

---

### 12. Web Search â€” `utils/web_search.py`

DuckDuckGo-powered web search fallback (free, no API key required).

| Method | Purpose |
|---|---|
| `search()` | Raw search â†’ list of `{title, url, snippet}` |
| `search_and_summarize()` | Formatted markdown context for RAG prompt injection |
| `search_health_topics()` | Health-focused search with enhanced keywords |

**Trigger**: Activated by the RAG pipeline when local context quality is below `web_search_quality_threshold` (default 0.65) or fewer than `min_context_threshold` items (default 2) are retrieved.

---

### 13. Validators â€” `utils/validators.py`

Response safety and quality validation.

| Check | Description |
|---|---|
| Safety check | Flags unsafe keywords (diagnosis, treatment plan, prescription, etc.) |
| Length check | Ensures response is between 50â€“1500 characters |
| Relevance check | Compares response keywords against query keywords |

**Note**: Validation is currently **disabled** (`enable_validation: false` in config) to reduce API latency during development.

---

### 14. Logger â€” `utils/logger.py`

Structured logging via Loguru with console + file output.

- **Console**: Colorized human-readable format
- **File**: `logs/agentic_rag.log` with 10MB rotation, 30-day retention, zip compression
- **Context manager**: `LogContext` binds `user_id`, `session_id` to log entries

---

## Agentic Pipeline â€” Detailed Flow

This is the complete query processing flow when a user sends a message:

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ORCHESTRATOR ANALYSIS                               â”‚
â”‚    â€¢ Build analysis prompt (system + user query)       â”‚
â”‚    â€¢ Call Gemini (temp=0.1, max_tokens=500)             â”‚
â”‚    â€¢ Parse JSON decision                               â”‚
â”‚    â€¢ Output: action, confidence, parameters             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚              â”‚
    â–¼           â–¼              â–¼
RETRIEVE    CALL_LLM      HYBRID
DOCUMENT/   (direct)      (retrieve +
MEMORY                     generate)
    â”‚           â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. QUERY PROCESSING                                    â”‚
â”‚    â€¢ If enable_query_expansion: LLM rewrites query     â”‚
â”‚    â€¢ Cleaned, expanded query passed to retrieval       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. HYBRID CONTEXT RETRIEVAL                            â”‚
â”‚    Source A: Memory (conversations collection)         â”‚
â”‚    Source B: Documents (document chunks collection)     â”‚
â”‚    Source C: Chat Summaries (session summaries)         â”‚
â”‚    â€¢ All 3 sources searched in parallel                 â”‚
â”‚    â€¢ Results merged and sorted by similarity            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. QUALITY ASSESSMENT + QUERY REFORMULATION LOOP       â”‚
â”‚    â€¢ Compute avg similarity of retrieved context       â”‚
â”‚    â€¢ If avg < reformulation_quality_threshold (0.3):   â”‚
â”‚      â†’ LLM rewrites query (up to 2 attempts)           â”‚
â”‚      â†’ Re-run retrieval with reformulated query        â”‚
â”‚    â€¢ If still poor â†’ trigger web search fallback       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. WEB SEARCH FALLBACK (if enabled)                    â”‚
â”‚    â€¢ Triggered when:                                   â”‚
â”‚      - avg similarity < web_search_quality_threshold   â”‚
â”‚      - OR fewer than min_context_threshold items       â”‚
â”‚    â€¢ DuckDuckGo search â†’ results injected into prompt  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. PROMPT BUILDING                                     â”‚
â”‚    â€¢ System prompt (from config)                       â”‚
â”‚    â€¢ Retrieved context (documents, memory, summaries)  â”‚
â”‚    â€¢ Web search results (if any)                       â”‚
â”‚    â€¢ Conversation history                              â”‚
â”‚    â€¢ User query                                        â”‚
â”‚    â€¢ Response guidelines + source priority instructions â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. LLM RESPONSE GENERATION                            â”‚
â”‚    â€¢ Call Gemini (temp=0.7, max_tokens=2000)            â”‚
â”‚    â€¢ Full prompt with all context                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. ITERATIVE REFLECTION (if enabled)                   â”‚
â”‚    â€¢ LLM checks if response is grounded in context     â”‚
â”‚    â€¢ Returns: is_grounded, issues, revised_answer      â”‚
â”‚    â€¢ If not grounded â†’ uses revised answer             â”‚
â”‚    â€¢ Max 1 reflection iteration (configurable)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. MEMORY STORAGE                                      â”‚
â”‚    â€¢ Store interaction (query + response) in memory    â”‚
â”‚    â€¢ Save turn to session JSON file                    â”‚
â”‚    â€¢ Periodic summarization if threshold reached       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
           User Response
```

---

## Vector Database & Chunking Architecture

### Document Processing Pipeline

```
Document Upload â†’ Text Extraction â†’ Chunking â†’ Embedding â†’ ChromaDB Storage
     â†“                â†“              â†“           â†“              â†“
  PDF/Word/Img    pypdf/OCR    Overlapping   MiniLM-L6-v2   Chunks with
   files         extraction    chunks        (384-dim)       rich metadata
```

### Chunking System

**Location**: `memory/document_store.py` & `utils/document_loader.py`

#### Algorithm
```python
def _chunk_text(text: str, chunk_size: int = 1500, overlap: int = 300) -> List[str]:
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap  # Overlap ensures context continuity
    return chunks
```

#### Configuration (`config/config.yaml`)
```yaml
chunking:
  enable_chunking: true
  chunk_size: 1500          # Characters per chunk
  chunk_overlap: 300        # Overlap between consecutive chunks
  min_chunk_size: 300       # Documents smaller than this â†’ single chunk
  chunk_search_multiplier: 3  # Fetch 3Ã— top_k for deduplication headroom
```

#### Chunk Metadata
Each chunk is stored with:
```python
{
    "user_id": "web_user",
    "filename": "document.pdf",
    "chunk_number": 0,           # 0-indexed
    "total_chunks": 5,
    "chunk_type": "chunked",     # or "single"
    "start_position": 0,
    "end_position": 1500,
    "content_length": 1500,
    "timestamp": "2026-02-28T...",
    "document_type": "uploaded_knowledge"
}
```

### Search & Deduplication

1. **Query embedding**: Convert query â†’ 384-dim vector
2. **Vector search**: ChromaDB similarity search (top_k Ã— chunk_search_multiplier)
3. **Deduplication**: Group chunks by document, keep top `max_chunks_per_document` (default 3) per document
4. **Context assembly**: Merge selected chunks for prompt injection

```yaml
rag:
  top_k_documents: 8
  similarity_threshold: 0.1
  max_chunks_per_document: 3
```

### ChromaDB Collections

| Collection | Purpose | Similarity Formula |
|---|---|---|
| `kinetichat_memory` | Conversation history | `max(0.0, 1.0 - (distance / 2.0))` |
| `kinetichat_memory_documents` | Document chunks | `max(0.0, 1.0 - (distance / 2.0))` |
| `kinetichat_chat_summaries` | Session summaries | `max(0.0, 1.0 - (distance / 2.0))` |

**Storage**: `data/vector_store/` (persistent across restarts via `chromadb.PersistentClient`)

---

## Session & Memory Management

### Session Flow (in `ui.py`)

```
User opens app
    â”‚
    â”œâ”€ init_session_state() â†’ loads cached resources (embedding model, vector store, etc.)
    â”‚
    â”œâ”€ _ensure_session() â†’ creates/loads session via SessionStore
    â”‚
    â”œâ”€ User sends message
    â”‚   â”œâ”€ process_user_query() â†’ orchestrator â†’ RAG pipeline â†’ response
    â”‚   â”œâ”€ add_message_to_chat() â†’ saves to session JSON file
    â”‚   â””â”€ Store interaction in memory (vector store)
    â”‚
    â”œâ”€ User starts new session
    â”‚   â”œâ”€ _summarize_current_session() â†’ SummarizeAgent condenses prior session
    â”‚   â”œâ”€ Summary embedded in chat_summaries collection
    â”‚   â””â”€ _start_new_session() â†’ creates fresh session file
    â”‚
    â””â”€ Session history sidebar â†’ list/switch/delete sessions
```

### Session Pruning
```yaml
memory:
  max_chat_sessions: 5    # Oldest sessions auto-deleted beyond this limit
```

### Summarization Pipeline
1. When user switches sessions, `_summarize_current_session()` is called
2. If session has â‰¥ 2 messages and hasn't been summarized:
   - `SummarizeAgent.summarize_session()` calls Gemini to generate a summary
   - Summary is embedded via `SummarizeAgent.store_summary()` into `chat_summaries` collection
   - Session marked as `is_summarized = true` in JSON file
3. Future queries can retrieve relevant past session summaries via `VectorStore.search_chat_summaries()`

---

## Web Search Fallback

The system uses DuckDuckGo as a free web search fallback when local knowledge is insufficient.

### Trigger Conditions (any of):
- Average similarity of retrieved context < `web_search_quality_threshold` (0.65)
- Fewer than `min_context_threshold` (2) context items retrieved

### Configuration
```yaml
rag:
  enable_web_search: true
  web_search_quality_threshold: 0.65
  min_context_threshold: 2
  max_web_results: 5
  web_search_timeout: 10  # seconds
```

### Integration Point
In `rag_pipeline.py â†’ generate_response()`:
```python
if web_search_triggered:
    web_context = web_search_service.search_and_summarize(query)
    # Web results injected as "ğŸŒ WEB SEARCH RESULTS" section in prompt
```

The LLM is instructed to cite web sources with URLs when using web search results.

---

## API Key Rotation System

### Setup
```bash
# .env file â€” comma-separated keys
GEMINI_API_KEYS=AIza..._key1,AIza..._key2,AIza..._key3
```

### Flow
```
API Call â†’ 200 OK â†’ reset_success() (reset failure counter)
    â”‚
    â””â†’ 429 Quota Error â†’ rotate_to_next_key()
                              â”‚
                              â”œâ”€ Cycles < 2 Ã— total_keys â†’ retry with next key
                              â”‚
                              â””â”€ Cycles exhausted â†’ raise error
```

### Thread Safety
- Singleton pattern with `threading.Lock()`
- All state mutations are thread-safe
- Used by `GeminiClient._rotate_and_retry()`

---

## Prompt Engineering

All prompts are centralized in `utils/prompt_templates.py`. The system uses 7 prompt categories:

| Category | Variable | Used By |
|---|---|---|
| `ORCHESTRATOR_PROMPTS` | system, decision_format | `agents/orchestrator.py` |
| `RAG_PROMPTS` | system, with_context | `retrieval/rag_pipeline.py` |
| `VALIDATION_PROMPTS` | safety_check | `utils/validators.py` |
| `SUMMARIZATION_PROMPTS` | conversation_summary, user_profile | `memory/memory_manager.py` |
| `QUERY_REFORMULATION_PROMPTS` | reformulate | `retrieval/rag_pipeline.py` |
| `REFLECTION_PROMPTS` | reflect | `retrieval/rag_pipeline.py` |
| `SESSION_SUMMARY_PROMPTS` | system, summarize | `agents/summarize_agent.py` |
| `FALLBACK_MESSAGES` | generic, memory_failure, etc. | Various |

### Source Priority (built into system prompt)
```
1. UPLOADED DOCUMENTS â€” Primary knowledge source
   â†’ "Based on [filename], ..."
2. WEB SEARCH RESULTS â€” When documents don't have the answer
   â†’ Always cite source URLs
3. GENERAL KNOWLEDGE â€” Only when neither is available
```

### Language Adaptation
The system prompt instructs the LLM to respond in the same language as the user's query (supports Vietnamese and English natively).

---

## Configuration Reference

**File**: `config/config.yaml` (226 lines)

### Orchestrator
```yaml
orchestrator:
  model: "gemini-2.5-flash"
  temperature: 0.1          # Low for consistent routing
  max_tokens: 500
  memory_retrieval_threshold: 0.6
  llm_call_threshold: 0.7
  motion_generation_threshold: 0.8
```

### LLM (Response Generation)
```yaml
llm:
  model: "gemini-2.5-flash"
  temperature: 0.7
  max_tokens: 2000
  enable_validation: false   # Disabled for debugging
  max_retries: 1
```

### Embedding
```yaml
embedding:
  model: "sentence-transformers/all-MiniLM-L6-v2"
  dimension: 384
  batch_size: 32
```

### RAG Pipeline
```yaml
rag:
  top_k_documents: 8
  similarity_threshold: 0.1
  max_chunks_per_document: 3
  enable_query_expansion: true
  enable_query_reformulation: true
  max_reformulation_attempts: 2
  reformulation_quality_threshold: 0.3
  enable_iterative_reflection: true
  max_reflection_iterations: 1
  enable_web_search: true
  web_search_quality_threshold: 0.65
  min_context_threshold: 2
  max_web_results: 5
```

### Memory
```yaml
memory:
  max_items_per_user: 100
  retention_days: 90
  top_k: 5
  similarity_threshold: 0.3
  summarization_interval: 5
  max_chat_sessions: 5
```

### Chunking
```yaml
chunking:
  enable_chunking: true
  chunk_size: 1500
  chunk_overlap: 300
  min_chunk_size: 300
  chunk_search_multiplier: 3
```

### Validation
```yaml
validation:
  enable_safety_check: true
  enable_factuality_check: false
  enable_relevance_check: true
  min_response_length: 50
  max_response_length: 1500
```

### Performance
```yaml
performance:
  enable_caching: true
  cache_ttl: 3600
  orchestrator_timeout: 5
  memory_retrieval_timeout: 3
  llm_timeout: 30
```

---

## Environment Variables

**File**: `.env` (see `.env.example` for template)

| Variable | Required | Description |
|---|---|---|
| `GEMINI_API_KEYS` | âœ… | Comma-separated Gemini API keys for rotation |
| `ORCHESTRATOR_MODEL` | âŒ | Override orchestrator model (default: from config.yaml) |
| `LLM_MODEL` | âŒ | Override LLM model (default: from config.yaml) |
| `EMBEDDING_MODEL` | âŒ | Override embedding model |
| `VECTOR_DB_TYPE` | âŒ | `chromadb` or `qdrant` |
| `VECTOR_DB_PATH` | âŒ | Path for ChromaDB persistence |
| `QDRANT_URL` | âŒ | Qdrant server URL |
| `QDRANT_API_KEY` | âŒ | Qdrant API key |
| `LOG_LEVEL` | âŒ | Logging level (DEBUG/INFO/WARNING/ERROR) |

**Priority**: Environment variables override `config.yaml` values (handled in `config/__init__.py`).

---

## Setup & Installation

### Prerequisites
- Python 3.10+ (tested with 3.13)
- Gemini API key(s) from [Google AI Studio](https://aistudio.google.com/app/apikey)
- *Optional*: Tesseract OCR for scanned document support

### Quick Setup (Windows â€” PowerShell)
```powershell
cd agentic_rag_gemini

# Option A: Automated
.\setup.ps1

# Option B: Manual
pip install -r requirements.txt
mkdir -Force data\vector_store, logs
cp .env.example .env
# Edit .env â†’ add GEMINI_API_KEYS=your_key_here
```

### Quick Setup (Linux/macOS)
```bash
cd agentic_rag_gemini
pip install -r requirements.txt
mkdir -p data/vector_store logs
cp .env.example .env
# Edit .env â†’ add GEMINI_API_KEYS=your_key_here
```

### OCR Setup (Optional)
```bash
# Windows (via Chocolatey)
choco install tesseract

# Linux
sudo apt-get install tesseract-ocr

# Verify
python -c "import pytesseract; print(pytesseract.get_tesseract_version())"
```

---

## Running the System

### Web Interface (Recommended)
```bash
python run_ui.py
# OR directly:
streamlit run ui.py
# Opens at http://localhost:8501
```

### CLI Interactive Mode
```bash
python main.py --mode interactive
```

### Single Query (Programmatic)
```python
from main import AgenticRAGSystem

system = AgenticRAGSystem()
result = system.process_query(
    query="What are the evaluation criteria?",
    user_id="web_user"
)
print(result["response"])
```

---

## Testing

### Run Tests
```bash
# Orchestrator tests
python -m pytest tests/test_orchestrator.py -v

# Session store tests
python -m pytest tests/test_session_store.py -v
```

### Test Individual Components
```bash
# Test embedding service
python -c "from memory.embedding_service import EmbeddingService; e = EmbeddingService(); print(len(e.embed_texts('test')))"

# Test vector store
python -c "from memory.vector_store import VectorStore; v = VectorStore(); print('Collections initialized')"

# Test Gemini API connection
python -c "from utils.gemini_client import GeminiClientWrapper; c = GeminiClientWrapper(); print(c.chat.completions.create(model='gemini-2.5-flash', messages=[{'role': 'user', 'content': 'hi'}]).choices[0].message.content)"

# Test document loader
python -c "from utils.document_loader import DocumentLoader; d = DocumentLoader(); print('Supported:', d.SUPPORTED_FORMATS)"

# Test web search
python -c "from utils.web_search import WebSearchService; s = WebSearchService(); print('Available:', s.is_available())"
```

---

## How to Extend the System

### Add a new orchestrator action type
1. Add to `ActionType` enum in `agents/orchestrator.py`
2. Update `OrchestratorAgent._build_analysis_prompt()` to include the new action
3. Handle the new action in `main.py:AgenticRAGSystem.process_query()`
4. Update ORCHESTRATOR_PROMPTS in `utils/prompt_templates.py`

### Change the LLM model
1. Update `config/config.yaml` â†’ `llm.model` and/or `orchestrator.model`
2. Available models: `gemini-2.5-flash`, `gemini-2.5-pro`, `gemini-2.0-flash`, `gemini-3-pro-preview`

### Change the embedding model
1. Update `config/config.yaml` â†’ `embedding.model` and `embedding.dimension`
2. Update `memory/embedding_service.py` â†’ `_init_sentence_transformer()`
3. **Critical**: Reset vector store (`VectorStore.reset_collections()`) since embeddings are dimension-dependent

### Add a new document format
1. Add the extension to `DocumentLoader.SUPPORTED_FORMATS`
2. Implement `_load_<format>()` method in `utils/document_loader.py`
3. Add the routing in `DocumentLoader.load_file()`

### Switch vector database to Qdrant
1. Set `VECTOR_DB_TYPE=qdrant` in `.env`
2. Start Qdrant server: `docker run -p 6333:6333 qdrant/qdrant`
3. Complete `VectorStore._init_qdrant()` implementation (currently a stub)

### Add response post-processing
1. Edit `retrieval/rag_pipeline.py` â†’ add logic after `_generate_llm_response()`
2. Or add a new validation rule in `utils/validators.py`

### Add a new prompt template
1. Add the template dict to `utils/prompt_templates.py`
2. Use `format_prompt()` or `get_prompt()` to retrieve it

---

## Dependency Matrix

### Core Dependencies (as of 2026-02-28)

| Package | Required Version | Installed | Purpose |
|---|---|---|---|
| streamlit | â‰¥ 1.28.0 | 1.54.0 | Web interface |
| streamlit-chat | â‰¥ 0.1.0 | âœ”ï¸ | Chat bubbles |
| pydantic | â‰¥ 2.0.0 | 2.12.4 | Config models |
| pydantic-settings | â‰¥ 2.0.0 | 2.12.0 | Settings management |
| python-dotenv | â‰¥ 1.0.0 | âœ”ï¸ | .env file loading |
| PyYAML | â‰¥ 6.0 | âœ”ï¸ | Config file parsing |
| chromadb | â‰¥ 0.6.0 | 1.5.1 | Vector database |
| qdrant-client | â‰¥ 1.6.0 | 1.17.0 | Vector DB (alt) |
| google-generativeai | â‰¥ 0.3.0 | 0.8.6 | Gemini API âš ï¸ |
| sentence-transformers | â‰¥ 2.2.0 | 5.2.3 | Embeddings |
| numpy | â‰¥ 1.24.0 | 2.4.2 | Numeric operations |
| tiktoken | â‰¥ 0.5.0 | âœ”ï¸ | Token counting |
| pypdf | â‰¥ 3.17.0 | 6.7.4 | PDF text extraction |
| python-docx | â‰¥ 0.8.11 | âœ”ï¸ | Word documents |
| pytesseract | â‰¥ 0.3.10 | âœ”ï¸ | OCR |
| pdf2image | â‰¥ 1.16.0 | âœ”ï¸ | PDF â†’ image for OCR |
| Pillow | â‰¥ 9.5.0 | âœ”ï¸ | Image processing |
| loguru | â‰¥ 0.7.0 | âœ”ï¸ | Structured logging |
| colorlog | â‰¥ 6.7.0 | âœ”ï¸ | Colored log output |
| ddgs | â‰¥ 9.0.0 | âœ”ï¸ | DuckDuckGo search |

### âš ï¸ Deprecation Notice

**`google-generativeai`** (v0.8.6) is deprecated. Google has ended support for this package.
- **Action required**: Migrate to **`google-genai`** (the new official SDK)
- **Impact**: No breaking changes currently, but no future bug fixes or updates
- **Migration guide**: https://github.com/google-gemini/deprecated-generative-ai-python

### Verified Compatibility (2026-02-28)

```
âœ… pydantic 2.12 + chromadb 1.5  â†’ Compatible
âœ… grpcio 1.78 + protobuf 5.29   â†’ Compatible
âœ… numpy 2.4 + sentence-transformers 5.2 â†’ Compatible
âœ… pip check â†’ "No broken requirements found"
```

---

## Resolved Issues & Fixes

### âœ… ChromaDB Persistence Issue
**Problem**: Documents lost between sessions (using in-memory client)  
**Fix**: Changed `chromadb.Client()` â†’ `chromadb.PersistentClient(path="data/vector_store/")`  
**File**: `memory/vector_store.py` â†’ `_init_chromadb()`

### âœ… ChromaDB Query Filters
**Problem**: Query failed with multiple metadata filter conditions  
**Fix**: Used `$and` operator for compound filters  
**File**: `memory/vector_store.py` â†’ `search()` and `search_documents()`

### âœ… Negative Similarity Scores
**Problem**: Similarity scores went negative with good embeddings  
**Root cause**: Incorrect distance-to-similarity conversion  
**Fix**: Changed `1 - distance` â†’ `max(0.0, 1.0 - (distance / 2.0))`  
**File**: `memory/vector_store.py`

### âœ… Pydantic v1/v2 ConfigError with ChromaDB
**Problem**: `pydantic.v1.errors.ConfigError` on startup  
**Root cause**: Old ChromaDB versions bundled pydantic v1 compatibility layer that clashed with pydantic v2  
**Fix**: Upgraded chromadb to â‰¥ 1.5.1  
**File**: `requirements.txt`

### âœ… Document Context Truncation
**Problem**: Document content cut off too early in prompts  
**Fix**: Increased content limit to 800 characters + added keyword-based sentence extraction  
**File**: `retrieval/rag_pipeline.py` â†’ `_build_prompt()`

### âœ… Embedding Normalization Conflict
**Problem**: Normalized embeddings + ChromaDB's L2 distance = incorrect similarities  
**Fix**: Removed manual normalization in EmbeddingService  
**File**: `memory/embedding_service.py`

### âœ… Web Search Import Issue
**Problem**: `ddgs` package import failed under Streamlit's ScriptRunner  
**Fix**: Lazy-load `DDGS` class at call time instead of module-level import  
**File**: `utils/web_search.py` â†’ `_load_ddgs_class()`

---

## Known Limitations & Open Issues

### âš ï¸ `google-generativeai` Deprecation
**Status**: Working but deprecated  
**Impact**: No future updates  
**Action**: Plan migration to `google-genai` SDK

### âš ï¸ Qdrant Backend is a Stub
**Status**: `VectorStore._init_qdrant()` exists but is not fully implemented  
**Impact**: Setting `VECTOR_DB_TYPE=qdrant` will not work end-to-end

### âš ï¸ Response Validation Disabled
**Status**: `enable_validation: false` in config  
**Reason**: Was adding latency and occasionally blocking valid responses during development  
**Action**: Re-enable and tune validation rules before production

### âš ï¸ No Authentication
**Status**: Web UI uses hardcoded `"web_user"` for all sessions  
**Impact**: No multi-user isolation  
**Action**: Add session-based auth before production deployment

### âš ï¸ Single-Language Embedding Model
**Status**: `all-MiniLM-L6-v2` is English-optimized  
**Impact**: Vietnamese document retrieval may have lower similarity scores  
**Action**: Consider multilingual models (e.g., `paraphrase-multilingual-MiniLM-L12-v2`)

---

## Production Deployment Checklist

| # | Item | Status |
|---|---|---|
| 1 | Use environment variables for all secrets | âœ… Done |
| 2 | Set `log_level: WARNING` in config | â¬œ TODO |
| 3 | Enable response validation | â¬œ TODO |
| 4 | Increase `max_retries` for stability | â¬œ TODO |
| 5 | Add authentication to web interface | â¬œ TODO |
| 6 | Monitor `logs/` directory (set up alerting) | â¬œ TODO |
| 7 | Use persistent vector store | âœ… Done |
| 8 | Handle API rate limiting gracefully | âœ… Done (key rotation) |
| 9 | Migrate to `google-genai` SDK | â¬œ TODO |
| 10 | Consider multilingual embedding model | â¬œ TODO |
| 11 | Implement proper error pages in Streamlit | â¬œ TODO |
| 12 | Set up CI/CD pipeline with tests | â¬œ TODO |

---

## Quick Reference â€” Common Modifications

| What to Change | Where |
|---|---|
| LLM model | `config/config.yaml` â†’ `llm.model` |
| Orchestrator model | `config/config.yaml` â†’ `orchestrator.model` |
| Response creativity | `config/config.yaml` â†’ `llm.temperature` |
| Embedding model | `config/config.yaml` â†’ `embedding.model` + `embedding.dimension` |
| Number of retrieved documents | `config/config.yaml` â†’ `rag.top_k_documents` |
| Similarity threshold | `config/config.yaml` â†’ `rag.similarity_threshold` |
| Chunk size | `config/config.yaml` â†’ `chunking.chunk_size` |
| Max sessions kept | `config/config.yaml` â†’ `memory.max_chat_sessions` |
| Web search aggressiveness | `config/config.yaml` â†’ `rag.web_search_quality_threshold` |
| Prompt templates | `utils/prompt_templates.py` |
| System persona | `config/config.yaml` â†’ `llm.system_prompt` |
| Routing behavior | `config/config.yaml` â†’ `orchestrator.system_prompt` |
| Safety keywords | `config/config.yaml` â†’ `validation.unsafe_keywords` |
| API keys | `.env` â†’ `GEMINI_API_KEYS` |
| UI layout | `ui.py` |
| Add document format | `utils/document_loader.py` â†’ `SUPPORTED_FORMATS` + new `_load_*()` |

---

## Contact / Further Reading

- **User setup**: See [`QUICKSTART.md`](QUICKSTART.md)
- **Config details**: See [`config/config.yaml`](config/config.yaml)
- **Prompt templates**: See [`utils/prompt_templates.py`](utils/prompt_templates.py)
- **Individual modules**: Each file contains detailed docstrings and method documentation
